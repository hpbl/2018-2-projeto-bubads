<p><img alt="Carthage compatible" src="https://img.shields.io/badge/Carthage-compatible-4BC51D.svg?style=flat" />
<img alt="CocoaPods Compatible" src="https://img.shields.io/cocoapods/v/SwiftOCR.svg" />
<img alt="Platform" src="https://img.shields.io/cocoapods/p/SwiftOCR.svg?style=flat" /></p>
<h1>SwiftOCR</h1>
<p>SwiftOCR is a fast and simple OCR library written in Swift. It uses a neural network for image recognition.
As of now, SwiftOCR is optimized for recognizing short, one line long alphanumeric codes (e.g. DI4C9CM). We currently support iOS and OS X.</p>
<h2>Features</h2>
<ul>
<li>[x] Easy to use training class</li>
<li>[x] High accuracy</li>
<li>[x] Great default image preprocessing</li>
<li>[x] Fast and accurate character segmentation algorithm</li>
<li>[x] Add support for lowercase characters</li>
<li>[x] Add support for connected character segmentation</li>
</ul>
<h2>Why should I choose SwiftOCR instead of Tesseract?</h2>
<p>This is a really good question. </p>
<p>If you want to recognize normal text like a poem or a news article, go with Tesseract, but if you want to recognize short, alphanumeric codes (e.g. gift cards), I would advise you to choose SwiftOCR because that's where it exceeds.</p>
<p>Tesseract is written in C++ and over 30 years old. To use it you first have to write a Objective-C++ wrapper for it. The main issue that's slowing down Tesseract is the way memory is managed. Too many memory allocations and releases slow it down.</p>
<p>I did some testing on over 50 difficult images containing alphanumeric codes. The results where astonishing. SwiftOCR beat Tesseract in every category.</p>
<p>|          | SwiftOCR  | Tesseract |
| -------- | :-------: | :-------: |
| Speed    | 0.08 sec. | 0.63 sec. |
| Accuracy | 97.7%     | 45.2%     |
| CPU      | ~30%      | ~90%      |
| Memory   | 45 MB     | 73 MB     |</p>
<h2>How does it work?</h2>
<p>First, SwiftOCR binarizes the input image. Afterwards it extracts the characters of the image using a technique called <a href="https://en.wikipedia.org/wiki/Connected-component_labeling">Connected-component labeling</a>. Finally the seperated characters get converted into numbers which then get feed into the neural network.</p>
<h2>How to use it?</h2>
<p>SwiftOCR is available through CocoaPods. To install it, simply add the following line to your Podfile:</p>
<p><code>pod 'SwiftOCR'</code></p>
<p>If you ever used Tesseract you know how exhausting it can be to implement OCR into your project. 
SwiftOCR is the exact opposite of Tesseract. It can be implemented using <strong>just 6 lines of code</strong>. </p>
<p>```swift
import SwiftOCR</p>
<p>let swiftOCRInstance = SwiftOCR()</p>
<p>swiftOCRInstance.recognize(myImage) { recognizedString in
    print(recognizedString)
}
```</p>
<p>To improve your experience with SwiftOCR you should set your Build Configuration to <code>Release</code>.</p>
<h4>Training</h4>
<p>Training SwiftOCR is pretty easy. There are only a few steps you have to do, before it can recognize a new font.</p>
<p>The easiest way to train SwiftOCR is using the training app that can be found under <code>/example/OS X/SwiftOCR Training</code>. First select the fonts you want to train from the list. After that, you can change the characters you want to train in the text field. Finally, you have to press the <code>Start Testing</code> button. The only thing that's left now, is waiting. Depending on your settings, this can take between a half and two minutes. After about two minutes you may manually stop the training.
Pressing the <code>Save</code> button will save trained network to your desktop.
The <code>Test</code> button is used for evaluating the accuracy of the trained neural network.</p>
<h2>Examples</h2>
<p>Here is an example image. SwiftOCR has no problem recognizing it. If you try to recognize the same image using Tesseract the output is 'LABMENSW' ?!?!?.</p>
<p><img alt="Image 1" src="https://github.com/garnele007/SwiftOCR/blob/master/example/OS%20X/SwiftOCR%20Example%20OS%20X/SwiftOCR%20Example%20OS%20X/images/Test%202.png?raw=true" /></p>
<p>This image is difficult to recognize because of two reasons:
- The lighting is uneven. This problem is solved by the innovative preprocessing algorithm of SwiftOCR.
- The text in this image is distorted. Since SwiftOCR uses a neural network for the recognition, this isn't a real problem. A NN is flexible like a human brain and can recognize even the most distorted image (most of the time).</p>
<h2>TODO</h2>
<ul>
<li>[ ] Port to <a href="https://github.com/BradLarson/GPUImage2">GPUImage 2</a></li>
</ul>
<h2>Dependencies</h2>
<ul>
<li><a href="https://github.com/collinhundley/Swift-AI">Swift-AI</a></li>
<li><a href="https://github.com/BradLarson/GPUImage">GPUImage</a></li>
<li><a href="https://github.com/hollance/swift-algorithm-club/tree/master/Union-Find">Union-Find</a></li>
</ul>
<h2>License</h2>
<pre><code>The code in this repository is licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

   http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
</code></pre>
<p><strong>NOTE</strong>: This software depends on other packages that may be licensed under different open source licenses.</p>